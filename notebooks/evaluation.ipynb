{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Q&A RAG Evaluation Using spaCy and NLI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the usage of a Contract Q&A system. The system reads contract documents, processes them, and answers queries about their content. Below are the steps followed:\n",
    "1. Load contract documents\n",
    "2. Chunk the documents\n",
    "3. Create a document store\n",
    "4. Perform evaluations using BLEU and Hallucination scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the spaCy model is installed\n",
    "import spacy\n",
    "\n",
    "def ensure_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        spacy.load(model_name)\n",
    "    except OSError:\n",
    "        from subprocess import run\n",
    "        run(f\"python -m spacy download {model_name}\", shell=True)\n",
    "\n",
    "ensure_spacy_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Libraries\n",
    "\n",
    "Here, the necessary libraries will be loaded and the spaCy model and the NLI model will be initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document as DocxDocument\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import sacrebleu\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load spaCy model for entity recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load NLI model\n",
    "nli_model = pipeline(\"text-classification\", model=\"roberta-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions\n",
    "\n",
    "Functions will be defined to: \n",
    "1. Read .docx files\n",
    "2. Load evaluation data.\n",
    "3. Calculate BLEU scores \n",
    "4. Extract entities\n",
    "5. Callculate Hallucination score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text from .docx files\n",
    "def read_docx(file_path):\n",
    "    doc = DocxDocument(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Function to load evaluation data\n",
    "def load_evaluation_data(file_path):\n",
    "    data = read_docx(file_path)\n",
    "    qa_pairs = []\n",
    "    lines = data.split('\\n')\n",
    "    current_question = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('Q') and ':' in line:\n",
    "            if current_question:\n",
    "                qa_pairs.append(current_question)\n",
    "            current_question = {\"question\": line.split(':', 1)[1].strip()}\n",
    "        elif line.startswith('A') and ':' in line and current_question:\n",
    "            current_question[\"answer\"] = line.split(':', 1)[1].strip()\n",
    "            qa_pairs.append(current_question)\n",
    "            current_question = None\n",
    "    return qa_pairs\n",
    "\n",
    "# Function to calculate BLEU score using sacrebleu\n",
    "def bleu(pred, ref):\n",
    "    return sacrebleu.sentence_bleu(pred, [ref]).score\n",
    "\n",
    "# Hallucination scoring functions\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents]\n",
    "\n",
    "def calculate_hallucination_score(generated_text, reference_text):\n",
    "    # Extract entities\n",
    "    gen_entities = extract_entities(generated_text)\n",
    "    ref_entities = extract_entities(reference_text)\n",
    "    \n",
    "    # Calculate entity overlap score\n",
    "    if gen_entities:\n",
    "        common_entities = set(gen_entities) & set(ref_entities)\n",
    "        entity_score = 1 - (len(common_entities) / len(set(gen_entities)))\n",
    "    else:\n",
    "        entity_score = 1  # Maximum hallucination if no entities are found in the generated text\n",
    "    \n",
    "    # Calculate NLI entailment score\n",
    "    nli_result = nli_model(f\"premise: {reference_text} hypothesis: {generated_text}\")\n",
    "    entailment_score = nli_result[0]['score'] if nli_result[0]['label'] == 'ENTAILMENT' else 0\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_score = (entity_score + (1 - entailment_score)) / 2\n",
    "    return combined_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function using BLEU and hallucination scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate RAG system\n",
    "def evaluate_rag_system(query_function, queries, references):\n",
    "    results = []\n",
    "    total_bleu_score = 0\n",
    "    total_hallucination_score = 0\n",
    "    num_samples = len(queries)\n",
    "    \n",
    "    for query, reference in zip(queries, references):\n",
    "        generated_answer = query_function(query)\n",
    "        bleu_score_value = bleu(generated_answer, reference)\n",
    "        hallucination_score_value = calculate_hallucination_score(generated_answer, reference)\n",
    "        \n",
    "        total_bleu_score += bleu_score_value\n",
    "        total_hallucination_score += hallucination_score_value\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"reference\": reference,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"bleu_score\": bleu_score_value,\n",
    "            \"hallucination_score\": hallucination_score_value\n",
    "        })\n",
    "    \n",
    "    avg_bleu_score = total_bleu_score / num_samples\n",
    "    avg_hallucination_score = total_hallucination_score / num_samples\n",
    "    \n",
    "    return results, {\n",
    "        \"average_bleu_score\": avg_bleu_score,\n",
    "        \"average_hallucination_score\": avg_hallucination_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Document Chunking and Creating Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk contracts\n",
    "def chunk_contracts(contracts):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    documents = [Document(page_content=chunk) for contract in contracts for chunk in splitter.split_text(contract)]\n",
    "    return documents\n",
    "\n",
    "# Function to create a document store\n",
    "def create_docstore(documents, openai_api_key):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    docstore = Chroma.from_documents(documents, embeddings)\n",
    "    return docstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "\n",
    "Load the OpenAI API key from the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Contracts\n",
    "\n",
    "Load and process the contract documents from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and load contracts\n",
    "CONTRACTS_DIR = \"../data/contracts\"\n",
    "def read_docx(file_path):\n",
    "    doc = DocxDocument(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_contracts():\n",
    "    contracts = []\n",
    "    for filename in os.listdir(CONTRACTS_DIR):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(CONTRACTS_DIR, filename)\n",
    "            contracts.append(read_docx(file_path))\n",
    "    return contracts\n",
    "\n",
    "def read_docx_from_file(file):\n",
    "    doc = DocxDocument(file)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_contracts_from_uploaded_files(uploaded_files):\n",
    "    contracts = [read_docx_from_file(file) for file in uploaded_files]\n",
    "    return contracts\n",
    "\n",
    "# Load and process contracts\n",
    "contracts = load_contracts()\n",
    "documents = chunk_contracts(contracts)\n",
    "docstore = create_docstore(documents, openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Data\n",
    "Load the evaluation data from a .docx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "EVAL_FILE = \"../data/qna/Robinson Q&A.docx\"\n",
    "evaluation_data = load_evaluation_data(EVAL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Query Function\n",
    "\n",
    "Define the query function to search for the most relevant answer in the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_function(query):\n",
    "    results = docstore.similarity_search(query, k=5)\n",
    "    return results[0].page_content if results else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Queries and References\n",
    "\n",
    "Prepare the queries and references from the evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare queries and references from evaluation data\n",
    "queries = [qa['question'] for qa in evaluation_data]\n",
    "references = [qa['answer'] for qa in evaluation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Run the evaluation and display the results for each query along with the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results\n",
      "Query: Who are the parties to the Agreement and what are their defined names?\n",
      "Reference: Cloud Investments Ltd. (“Company”) and Jack Robinson (“Advisor”)\n",
      "Generated Answer: Entire Agreement; No Waiver or Assignment: This Agreement together with the Exhibits, which are attached hereto and incorporated herein, set forth the entire Agreement between the parties and shall supersede all previous communications and agreements between the parties, either oral or written. This Agreement may be modified only by a written amendment executed by both parties. This Agreement may not be assigned, sold, delegated or transferred in any manner by Advisor for any reason whatsoever.\n",
      "BLEU Score: 0.50\n",
      "Hallucination Score: 0.88\n",
      "\n",
      "Query: What is the termination notice?\n",
      "Reference: According to section 4:14 days for convenience by both parties. The Company may terminate without notice if the Advisor refuses or cannot perform the Services or is in breach of any provision of this Agreement.\n",
      "Generated Answer: Termination: Either party, at any given time, may terminate this Agreement, for any reason whatsoever, with or without cause, upon fourteen (14) days’ prior written notice. Notwithstanding the above, the Company may terminate this Agreement immediately and without prior notice if Advisor refuses or is unable to perform the Services, or is in breach of any provision of this Agreement.\n",
      "BLEU Score: 20.92\n",
      "Hallucination Score: 0.80\n",
      "\n",
      "Query: What are the payments to the Advisor under the Agreement?\n",
      "Reference: According to section 6: 1. Fees of $9 per hour up to a monthly limit of $1,500, 2. Workspace expense of $100 per month, 3. Other reasonable and actual expenses if approved by the company in writing and in advance.\n",
      "Generated Answer: determined by any governmental authority that the Advisor and/or anyone on Advisor's behalf, is, regardless of the terms of this Agreement, an employee of the Company, then payments to Advisor hereunder shall be reduced effective retroactively as of the beginning of the Term so that 60% of such payments shall constitute salary payments, and 40% of such payments shall constitute payment by the Company for all other Advisor statutory rights and benefits as employee of the Company throughout the\n",
      "BLEU Score: 1.22\n",
      "Hallucination Score: 1.00\n",
      "\n",
      "Query: Can the Agreement or any of its obligations be assigned?\n",
      "Reference: 1. Under section 1.1 the Advisor can’t assign any of his obligations without the prior written consent of the Company, 2. Under section 9  the Advisor may not assign the Agreement and the Company may assign it, 3 Under section 9 of the Undertaking the Company may assign the Undertaking.\n",
      "Generated Answer: reason whatsoever. The Company may assign the Agreement to a successor of all or substantially all of its assets or business, provided the assignee has assumed the Company’s obligations under this Agreement.\n",
      "BLEU Score: 5.96\n",
      "Hallucination Score: 0.50\n",
      "\n",
      "Query: Who owns the IP?\n",
      "Reference: According to section 4 of the Undertaking (Appendix A), Any Work Product, upon creation, shall be fully and exclusively owned by the Company.\n",
      "Generated Answer: IP: Any Work Product, upon creation, shall be fully and exclusively owned by the Company. The Advisor, immediately upon Company’s request, shall sign any document and/or perform any action needed to formalize such ownership. The Advisor shall not obtain any rights in the Work Product, including moral rights and/or rights for royalties or other consideration under any applicable law (including Section 134 of the Israeli Patent Law – 1967 if applicable), and shall not be entitled to any\n",
      "BLEU Score: 18.77\n",
      "Hallucination Score: 0.90\n",
      "\n",
      "Query: Is there a non-compete obligation to the Advisor?\n",
      "Reference: Yes. During the term of engagement with the Company and for a period of 12 months thereafter.\n",
      "Generated Answer: Non-Compete: During the term of engagement with the Company and for a period of 12 months thereafter, Advisor shall not be involved, as an employee, owner, contractor or otherwise, in any business, which competes with the Company’s Business, and shall not solicit and/or hire any employee and/or service provider of the Company, without the prior written consent of the Company.\n",
      "BLEU Score: 20.25\n",
      "Hallucination Score: 0.75\n",
      "\n",
      "Query: Can the Advisor charge for meal time?\n",
      "Reference: No. See Section 6.1, Billable Hour doesn’t include meals or travel time.\n",
      "Generated Answer: As full and sole consideration for the Services, and subject to the performance of the Services, the Company shall pay the Advisor, against an invoice provided to the Company by Advisor, hourly fees at a rate of USD 9 (nine) per Billable Hour as defined below, limited to a maximum of USD 1,500 per month (the \"Fees\"). In addition, the Company shall pay the advisor USD 100 per month to finance a workspace for the Advisor, as long as the Advisor actually hires a professional workspace (the\n",
      "BLEU Score: 0.84\n",
      "Hallucination Score: 0.92\n",
      "\n",
      "Query: In which street does the Advisor live?\n",
      "Reference: 1 Rabin st, Tel Aviv, Israel\n",
      "Generated Answer: Jurisdiction: This Undertaking shall be governed by and construed in accordance with the laws of the State of Israel, without giving effect to its laws pertaining to conflict of laws.  The Advisor agrees that any and all disputes in connection with this Undertaking shall be submitted to the exclusive jurisdiction of the competent courts in the city of Tel Aviv-Yafo, Israel.\n",
      "BLEU Score: 1.30\n",
      "Hallucination Score: 0.88\n",
      "\n",
      "Query: Is the Advisor entitled to social benefits?\n",
      "Reference: No. According to section 8 of the Agreement, the Advisor is an independent consultant and shall not be entitled to any overtime pay, insurance, paid vacation, severance payments or similar fringe or employment benefits from the Company.\n",
      "Generated Answer: Advisor shall be solely responsible for any income taxes or other assessments made or imposed by any governmental authority on Advisor with respect to the Services rendered and the compensation received hereunder, and any and all expenses and costs of himself, employees, agents and representatives, including, without limitation, any salary, overtime, severance or social benefits payable thereto, and marketing costs incurred in connection with the performance of obligations hereunder.\n",
      "BLEU Score: 1.62\n",
      "Hallucination Score: 0.50\n",
      "\n",
      "Query: What happens if the Advisor claims compensation based on employment relationship with the Company?\n",
      "Reference: If the Advisor is determined to be an employee of the Company by a governmental authority, payments to the Advisor will be retroactively reduced so that 60% constitutes salary payments and 40% constitutes payment for statutory rights and benefits. The Company may offset any amounts due to the Advisor from any amounts payable under the Agreement. The Advisor must indemnify the Company for any losses or expenses incurred if an employer/employee relationship is determined to exist.\n",
      "Generated Answer: throughout the Term. Advisor further consents that the Company may offset any amounts due to him under this Section from any amounts payable to Advisor under this Agreement. Advisor shall indemnify the Company for any loss or expenses incurred by the Company if it were determined that an alleged employer/employee relationship existed between the Advisor and the Company.\n",
      "BLEU Score: 20.63\n",
      "Hallucination Score: 0.50\n",
      "\n",
      "Summary Evaluation Results\n",
      "Average BLEU Score: 9.20\n",
      "Average Hallucination Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results, evaluation_summary = evaluate_rag_system(query_function, queries, references)\n",
    "\n",
    "# Display detailed evaluation results\n",
    "print(\"Evaluation Results\")\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Reference: {result['reference']}\")\n",
    "    print(f\"Generated Answer: {result['generated_answer']}\")\n",
    "    print(f\"BLEU Score: {result['bleu_score']:.2f}\")\n",
    "    print(f\"Hallucination Score: {result['hallucination_score']:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Display summary evaluation results\n",
    "print(\"Summary Evaluation Results\")\n",
    "print(f\"Average BLEU Score: {evaluation_summary['average_bleu_score']:.2f}\")\n",
    "print(f\"Average Hallucination Score: {evaluation_summary['average_hallucination_score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
